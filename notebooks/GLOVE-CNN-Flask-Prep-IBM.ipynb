{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.models import save_model\n",
    "\n",
    "import keras\n",
    "import h5py\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "# from keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Input, InputLayer\n",
    "from keras.layers import Embedding, Activation, Dropout, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import itertools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_Path = 'https://kstonedev.s3-us-west-2.amazonaws.com/W266/USPTO-2M/'\n",
    "local_path = 'data/'\n",
    "all_files = [\n",
    "    '2006_USPTO.json',\n",
    "    '2007_USPTO.json',\n",
    "    '2008_USPTO.json',\n",
    "    '2009_USPTO.json',\n",
    "    '2010_USPTO.json',\n",
    "    '2011_USPTO.json',\n",
    "    '2012_USPTO.json',\n",
    "    '2013_USPTO.json',\n",
    "    '2014_USPTO.json',\n",
    "    '2015_USPTO.json'\n",
    "]\n",
    "\n",
    "# for file in all_files:\n",
    "#     wget.download(AWS_Path + file, out='.')\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting the following files:\n",
      " ['2006_USPTO.json', '2007_USPTO.json', '2008_USPTO.json', '2009_USPTO.json', '2010_USPTO.json', '2011_USPTO.json', '2012_USPTO.json', '2013_USPTO.json', '2014_USPTO.json', '2015_USPTO.json']\n"
     ]
    }
   ],
   "source": [
    "num_files_to_read = \"all\" # set to \"all\" or a number such as 2\n",
    "if num_files_to_read == \"all\":\n",
    "  print('Ingesting the following files:\\n', sorted(all_files))\n",
    "  patents = pd.concat(pd.read_json(local_path + 'USPTO-2M/' + f) for f in sorted(all_files))\n",
    "else:\n",
    "  print('Ingesting the following files:\\n', sorted(all_files)[-num_files_to_read:])\n",
    "  patents = pd.concat(pd.read_json(local_path + 'USPTO-2M/' + f) for f in sorted(all_files)[-num_files_to_read:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CRC labels 632\n"
     ]
    }
   ],
   "source": [
    "X = list(patents[\"Abstract\"])\n",
    "abstract_words = [x for sent in X for x in sent.split(' ')]\n",
    "unique_words = len(set(abstract_words))\n",
    "CRC_labels = patents.Subclass_labels\n",
    "# binarize labels into 1-hot encodings\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(CRC_labels)\n",
    "num_CRC_labels = len(y[0])\n",
    "\n",
    "print(\"Total number of CRC labels\", num_CRC_labels)\n",
    "\n",
    "# save CRC label list to a file for inference\n",
    "pd.DataFrame(mlb.classes_).to_csv('crc_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep full patent table intact for analysis at end\n",
    "P_train3, P_test3, y_train3, y_test3 = train_test_split(patents, y, test_size=0.20, random_state=42)\n",
    "X_train3 = list(P_train3['Abstract'])\n",
    "X_test3 = list(P_test3['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 3\n",
    "if experiment_number == 1:\n",
    "  (P_train, P_test, X_train, X_test, y_train, y_test) = (P_train1, P_test1, X_train1, X_test1, y_train1, y_test1)\n",
    "elif experiment_number == 2:\n",
    "  (P_train, P_test, X_train, X_test, y_train, y_test) = (P_train2, P_test2, X_train2, X_test2, y_train2, y_test2)\n",
    "elif experiment_number == 3:\n",
    "  (P_train, P_test, X_train, X_test, y_train, y_test) = (P_train3, P_test3, X_train3, X_test3, y_train3, y_test3)\n",
    "elif experiment_number == 4:\n",
    "  (P_train, P_test, X_train, X_test, y_train, y_test) = (P_train4, P_test4, X_train4, X_test4, y_train4, y_test4)\n",
    "else:\n",
    "  print(\"Unknown experiment number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to word embeddings\n",
    "\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=unique_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Top 10 words\n",
    "top10words = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "# print(\"Top 10 words:\\n\", top10words)\n",
    "\n",
    "# Convert text to sequence of numbers, each number representing a word\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GloVe word embeddings to convert text inputs to their numeric counterparts\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(local_path + 'glove.6B.100d.txt', encoding='utf8')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric f1 definition\n",
    "from keras import backend as K\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "def f1(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def weighted_bce(y_true, y_pred):\n",
    "    # weights become 2 if y_true is 1, and 1 if y_true is 0\n",
    "    weights = (y_true * 2.) + (1. - y_true)\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_bce = K.mean(bce * weights)\n",
    "    return weighted_bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "filters = 512\n",
    "hidden_dims = 512\n",
    "print('Build model...')\n",
    "input = keras.Input(shape=(maxlen,), name='input_embeddings')\n",
    "embedding = keras.layers.Embedding(vocab_size, 100,\n",
    "                    input_length=maxlen, weights=[embedding_matrix],\n",
    "                                             trainable=False)(input)\n",
    "conv0 = keras.layers.Conv1D(filters, 2, activation='relu')(embedding)\n",
    "maxpool0 = keras.layers.GlobalMaxPooling1D()(conv0)\n",
    "conv1 = keras.layers.Conv1D(filters, 3, activation='relu')(embedding)\n",
    "maxpool1 = keras.layers.GlobalMaxPooling1D()(conv1)\n",
    "conv2 = keras.layers.Conv1D(filters, 4, activation='relu')(embedding)\n",
    "maxpool2 = keras.layers.GlobalMaxPooling1D()(conv2)\n",
    "conv3 = keras.layers.Conv1D(filters, 5, activation='relu')(embedding)\n",
    "maxpool3 = keras.layers.GlobalMaxPooling1D()(conv3)\n",
    "concat1 = keras.layers.concatenate([maxpool0, maxpool1, maxpool2, maxpool3], axis=1)\n",
    "dropout1 = keras.layers.Dropout(rate=0.2)(concat1)\n",
    "dense = keras.layers.Dense(hidden_dims, activation='relu', name='dense')(dropout1)\n",
    "batchnorm = keras.layers.BatchNormalization()(dense)\n",
    "dense = keras.layers.Dropout(rate=0.5)(batchnorm)\n",
    "pred = keras.layers.Dense(num_CRC_labels, activation='sigmoid', name='crc')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_embeddings (InputLayer)   (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 100)     21382700    input_embeddings[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 199, 512)     102912      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 198, 512)     154112      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 512)     205312      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 196, 512)     256512      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 512)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 512)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 512)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 512)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2048)         0           global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2048)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          1049088     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "crc (Dense)                     (None, 632)          324216      dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,476,900\n",
      "Trainable params: 2,093,176\n",
      "Non-trainable params: 21,383,724\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss=weighted_bce,\n",
    "              optimizer='adam',\n",
    "              metrics=[f1,\n",
    "                      tf.keras.metrics.Precision(name='precision'),\n",
    "                      tf.keras.metrics.Precision(name='precision_1', top_k=1),\n",
    "                      tf.keras.metrics.Recall(name='recall'),\n",
    "                      tf.keras.metrics.Recall(name='recall_5', top_k=5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600117 samples, validate on 400030 samples\n",
      "Epoch 1/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0080 - f1: 0.5720 - precision: 0.6082 - precision_1: 0.6312 - recall: 0.4982 - recall_5: 0.8057 - val_loss: 0.0078 - val_f1: 0.5815 - val_precision: 0.6087 - val_precision_1: 0.6315 - val_recall: 0.4986 - val_recall_5: 0.8061\n",
      "Epoch 2/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0080 - f1: 0.5723 - precision: 0.6092 - precision_1: 0.6320 - recall: 0.4991 - recall_5: 0.8065 - val_loss: 0.0078 - val_f1: 0.5838 - val_precision: 0.6098 - val_precision_1: 0.6324 - val_recall: 0.4995 - val_recall_5: 0.8068\n",
      "Epoch 3/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0080 - f1: 0.5730 - precision: 0.6102 - precision_1: 0.6327 - recall: 0.5000 - recall_5: 0.8072 - val_loss: 0.0078 - val_f1: 0.5851 - val_precision: 0.6107 - val_precision_1: 0.6331 - val_recall: 0.5004 - val_recall_5: 0.8075\n",
      "Epoch 4/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0080 - f1: 0.5736 - precision: 0.6112 - precision_1: 0.6336 - recall: 0.5009 - recall_5: 0.8079 - val_loss: 0.0078 - val_f1: 0.5851 - val_precision: 0.6115 - val_precision_1: 0.6339 - val_recall: 0.5014 - val_recall_5: 0.8082\n",
      "Epoch 5/10\n",
      "1600117/1600117 [==============================] - 412s 258us/step - loss: 0.0079 - f1: 0.5741 - precision: 0.6119 - precision_1: 0.6342 - recall: 0.5018 - recall_5: 0.8085 - val_loss: 0.0077 - val_f1: 0.5874 - val_precision: 0.6123 - val_precision_1: 0.6346 - val_recall: 0.5023 - val_recall_5: 0.8088\n",
      "Epoch 6/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0079 - f1: 0.5743 - precision: 0.6127 - precision_1: 0.6350 - recall: 0.5027 - recall_5: 0.8091 - val_loss: 0.0077 - val_f1: 0.5873 - val_precision: 0.6130 - val_precision_1: 0.6353 - val_recall: 0.5031 - val_recall_5: 0.8094\n",
      "Epoch 7/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0079 - f1: 0.5756 - precision: 0.6134 - precision_1: 0.6356 - recall: 0.5036 - recall_5: 0.8096 - val_loss: 0.0077 - val_f1: 0.5899 - val_precision: 0.6137 - val_precision_1: 0.6360 - val_recall: 0.5040 - val_recall_5: 0.8100\n",
      "Epoch 8/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0079 - f1: 0.5750 - precision: 0.6141 - precision_1: 0.6363 - recall: 0.5045 - recall_5: 0.8104 - val_loss: 0.0077 - val_f1: 0.5881 - val_precision: 0.6143 - val_precision_1: 0.6366 - val_recall: 0.5049 - val_recall_5: 0.8105\n",
      "Epoch 9/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0079 - f1: 0.5762 - precision: 0.6146 - precision_1: 0.6370 - recall: 0.5053 - recall_5: 0.8108 - val_loss: 0.0077 - val_f1: 0.5876 - val_precision: 0.6149 - val_precision_1: 0.6372 - val_recall: 0.5057 - val_recall_5: 0.8111\n",
      "Epoch 10/10\n",
      "1600117/1600117 [==============================] - 413s 258us/step - loss: 0.0079 - f1: 0.5765 - precision: 0.6152 - precision_1: 0.6375 - recall: 0.5061 - recall_5: 0.8114 - val_loss: 0.0078 - val_f1: 0.5841 - val_precision: 0.6155 - val_precision_1: 0.6378 - val_recall: 0.5064 - val_recall_5: 0.8116\n"
     ]
    }
   ],
   "source": [
    "history3 = model.fit(X_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Saved CRC labels to disk\n",
      "Save tokenizer data to disk\n"
     ]
    }
   ],
   "source": [
    "# Save Model for inference\n",
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, local_path + 'model_allfiles.h5')\n",
    "print('Saved model to disk')\n",
    "\n",
    "# save CRC label list for inference\n",
    "path = local_path\n",
    "pd.DataFrame(mlb.classes_).to_csv(local_path + 'crc_labels_allfiles.csv')\n",
    "print('Saved CRC labels to disk')\n",
    "\n",
    "# !pip install pickle\n",
    "import pickle\n",
    "\n",
    "# saving tokenizer info for inference\n",
    "with open(local_path + 'tokenizer_allfiles.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Saved tokenizer data to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1163125727079847662\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1567043388891744622\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5823642204537443658\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15956161332\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6800377534166770804\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:07.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
